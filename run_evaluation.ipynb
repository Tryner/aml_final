{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "%cd /content/drive/MyDrive/aml_final/aml_final/\n",
    "! git pull\n",
    "! pip install setfit\n",
    "'''\n",
    "from datasets import load_dataset, Dataset\n",
    "from setfit import SetFitModel, Trainer, TrainingArguments\n",
    "import torch, gc\n",
    "\n",
    "from data.dataset_config import DatasetConfig\n",
    "from train.active_learning import ActiveTrainer, create_random_subset, add_random_samples\n",
    "from train.active_learning_config import ActiveLearningConfig\n",
    "from train.reporter import Reporter\n",
    "from train.metrics import camprehesive_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 6 #change\n",
    "samples_per_cycle = num_classes * 2\n",
    "dataset_name = \"dair-ai/emotion\" #change\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\" #change\n",
    "dataset = load_dataset(dataset_name) #change\n",
    "dataset_config = DatasetConfig(text_column=\"text\", num_classes=num_classes)\n",
    "\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"validation\"]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "final_reporter = Reporter(dataset_name + \"_final.csv\", label_column=dataset_config.label_column)\n",
    "cycle_reporter = Reporter(dataset_name + \"_cycle.csv\", report_train_args=False, label_column=dataset_config.label_column)\n",
    "def after_train_callback(trainer: Trainer, dataset: Dataset, run_id: int):\n",
    "    cycle_reporter.report(trainer=trainer, dataset=dataset, run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starts one run of active learning\n",
    "cycle_train_args = TrainingArguments(num_iterations=10, num_epochs=(1, 8))\n",
    "final_train_args = TrainingArguments(num_iterations=20, num_epochs=(1, 16))\n",
    "run_id = 0\n",
    "\n",
    "def run_train(initial_train_subset: Dataset, active_learning_config: ActiveLearningConfig, **kwargs):\n",
    "    global run_id\n",
    "    trainer = ActiveTrainer(\n",
    "        full_train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        initial_train_subset=initial_train_subset,\n",
    "        train_args=cycle_train_args,\n",
    "        active_learning_config=active_learning_config, \n",
    "        dataset_config=dataset_config,\n",
    "        metric=camprehesive_metrics,\n",
    "        run_id=run_id,\n",
    "        final_model_train_args=final_train_args,\n",
    "        #after_train_callback=after_train_callback, #just slows down training\n",
    "    )\n",
    "    if active_learning_config.active_sampling_strategy == \"random\": # speed up training, just train one cycle\n",
    "        trainer.train_subset = add_random_samples(train_dataset, initial_train_subset, samples_per_cycle * active_learning_config.active_learning_cycles, dataset_config, seed=run_id)\n",
    "        t = trainer.run_training(final_model=True)\n",
    "    else:\n",
    "        t = trainer.train()\n",
    "    final_reporter.report(\n",
    "        trainer=t, \n",
    "        dataset=trainer.train_subset, \n",
    "        active_learning_config=active_learning_config, \n",
    "        dataset_name=dataset_name, run_id=run_id, **kwargs\n",
    "        )\n",
    "    run_id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hyperparam_search(initial_train_subset: Dataset):\n",
    "    for setting in [\"random\", \"max_entropy\", \"max_entropy_balanced\"]:\n",
    "        if setting == \"random\":\n",
    "            strategy =  \"random\"\n",
    "            balance = 0.0\n",
    "        elif setting == \"max_entropy\":\n",
    "            strategy =  \"max_entropy\"\n",
    "            balance = 0.0\n",
    "        elif setting == \"max_entropy_balanced\":\n",
    "            strategy =  \"max_entropy\"\n",
    "            balance = 0.25\n",
    "        for unlabeled_samples in range(120, 700, 480):\n",
    "            config = ActiveLearningConfig(samples_per_cycle=samples_per_cycle, active_sampling_strategy=strategy, balancing_factor=balance, unlabeled_samples=unlabeled_samples, model_name=model_name)\n",
    "            run_train(initial_train_subset, config, setting=setting)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c21446bdaf4fc1a4ba7b563ed08362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "add_random_samples() missing 2 required positional arguments: 'label_column' and 'text_column'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset_seed \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      2\u001b[0m     initial_train_subset \u001b[38;5;241m=\u001b[39m create_random_subset(train_dataset, dataset_config, num_samples\u001b[38;5;241m=\u001b[39msamples_per_cycle, seed\u001b[38;5;241m=\u001b[39mdataset_seed)\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mrun_hyperparam_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_train_subset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m, in \u001b[0;36mrun_hyperparam_search\u001b[1;34m(initial_train_subset)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m unlabeled_samples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m120\u001b[39m, \u001b[38;5;241m700\u001b[39m, \u001b[38;5;241m480\u001b[39m):\n\u001b[0;32m     13\u001b[0m     config \u001b[38;5;241m=\u001b[39m ActiveLearningConfig(samples_per_cycle\u001b[38;5;241m=\u001b[39msamples_per_cycle, active_sampling_strategy\u001b[38;5;241m=\u001b[39mstrategy, balancing_factor\u001b[38;5;241m=\u001b[39mbalance, unlabeled_samples\u001b[38;5;241m=\u001b[39munlabeled_samples, model_name\u001b[38;5;241m=\u001b[39mmodel_name)\n\u001b[1;32m---> 14\u001b[0m     \u001b[43mrun_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_train_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msetting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msetting\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m, in \u001b[0;36mrun_train\u001b[1;34m(initial_train_subset, active_learning_config, **kwargs)\u001b[0m\n\u001b[0;32m      8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ActiveTrainer(\n\u001b[0;32m      9\u001b[0m     full_train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m     10\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m#after_train_callback=after_train_callback, #just slows down training\u001b[39;00m\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m active_learning_config\u001b[38;5;241m.\u001b[39mactive_sampling_strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;66;03m# speed up training, just train one cycle\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain_subset \u001b[38;5;241m=\u001b[39m \u001b[43madd_random_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_train_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples_per_cycle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mactive_learning_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_learning_cycles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     t \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mrun_training(final_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: add_random_samples() missing 2 required positional arguments: 'label_column' and 'text_column'"
     ]
    }
   ],
   "source": [
    "for dataset_seed in range(5):\n",
    "    initial_train_subset = create_random_subset(train_dataset, dataset_config, num_samples=samples_per_cycle, seed=dataset_seed)\n",
    "    run_hyperparam_search(initial_train_subset)\n",
    "samples_per_cycle = 4* samples_per_cycle # larger dataset\n",
    "for dataset_seed in range(5):\n",
    "    initial_train_subset = create_random_subset(train_dataset, dataset_config, num_samples=samples_per_cycle, seed=dataset_seed)\n",
    "    run_hyperparam_search(initial_train_subset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
