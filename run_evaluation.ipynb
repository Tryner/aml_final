{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "%cd /content/drive/MyDrive/aml_final/aml_final/\n",
    "! git pull\n",
    "! pip install setfit\n",
    "'''\n",
    "from datasets import load_dataset, Dataset\n",
    "from setfit import SetFitModel, Trainer, TrainingArguments\n",
    "import torch, gc\n",
    "\n",
    "from data.dataset_config import DatasetConfig\n",
    "from train.active_learning import ActiveTrainer, create_random_subset, add_random_samples\n",
    "from train.active_learning_config import ActiveLearningConfig\n",
    "from train.reporter import Reporter\n",
    "from train.metrics import camprehesive_metrics\n",
    "from data.load_datasets import load_spanish_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4 #change\n",
    "samples_per_cycle = num_classes * 2\n",
    "dataset_name = \"twitter_humor\"\n",
    "model_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\" #change\n",
    "dataset = load_spanish_dataset(\"twitter_humor\") #change\n",
    "dataset_config = DatasetConfig(text_column=\"tweet\", num_classes=num_classes) #change?\n",
    "\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"validation\"]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset_name = dataset_name.replace(\"/\", \"_\") # prevent file system errors\n",
    "final_reporter = Reporter(dataset_name + \"_final.csv\", label_column=dataset_config.label_column)\n",
    "cycle_reporter = Reporter(dataset_name + \"_cycle.csv\", report_train_args=False, label_column=dataset_config.label_column) # not used\n",
    "def after_train_callback(trainer: Trainer, dataset: Dataset, run_id: int):\n",
    "    cycle_reporter.report(trainer=trainer, dataset=dataset, run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starts one run of active learning\n",
    "cycle_train_args = TrainingArguments(num_iterations=10, num_epochs=(1, 8))\n",
    "final_train_args = TrainingArguments(num_iterations=20, num_epochs=(1, 16))\n",
    "run_id = 0\n",
    "\n",
    "def run_train(initial_train_subset: Dataset, active_learning_config: ActiveLearningConfig, **kwargs):\n",
    "    global run_id\n",
    "    trainer = ActiveTrainer(\n",
    "        full_train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        initial_train_subset=initial_train_subset,\n",
    "        train_args=cycle_train_args,\n",
    "        active_learning_config=active_learning_config, \n",
    "        dataset_config=dataset_config,\n",
    "        metric=camprehesive_metrics,\n",
    "        run_id=run_id,\n",
    "        final_model_train_args=final_train_args,\n",
    "        #after_train_callback=after_train_callback, #just slows down training\n",
    "    )\n",
    "    if active_learning_config.active_sampling_strategy == \"random\": # speed up training, just train one cycle\n",
    "        trainer.train_subset = add_random_samples(train_dataset, initial_train_subset, len(initial_train_subset) + (samples_per_cycle * active_learning_config.active_learning_cycles), dataset_config, seed=run_id)\n",
    "        t = trainer.run_training(final_model=True)\n",
    "    else:\n",
    "        t = trainer.train()\n",
    "    final_reporter.report(\n",
    "        trainer=t, \n",
    "        dataset=trainer.train_subset, \n",
    "        active_learning_config=active_learning_config, \n",
    "        dataset_name=dataset_name, run_id=run_id, **kwargs\n",
    "        )\n",
    "    run_id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hyperparam_search(initial_train_subset: Dataset):\n",
    "    for setting in [\"random\", \"max_entropy\", \"max_entropy_balanced\"]:\n",
    "        if setting == \"random\":\n",
    "            strategy =  \"random\"\n",
    "            balance = 0.0\n",
    "        elif setting == \"max_entropy\":\n",
    "            strategy =  \"max_entropy\"\n",
    "            balance = None\n",
    "        elif setting == \"max_entropy_balanced\":\n",
    "            strategy =  \"max_entropy\"\n",
    "            balance = 0.25\n",
    "        unlabeled_samples = 10 * samples_per_cycle\n",
    "        config = ActiveLearningConfig(samples_per_cycle=samples_per_cycle, active_sampling_strategy=strategy, balancing_factor=balance, unlabeled_samples=unlabeled_samples, model_name=model_name)\n",
    "        run_train(initial_train_subset, config, setting=setting)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48966e287404a8498973ccbabc30605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c697b18efcac40068e53c0a0cfbc8dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a98d1e6898e4a939a0cd2627cf64e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef6f559bcc048df89e1f4b307b1af95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying column mapping to the training dataset\n",
      "Applying column mapping to the evaluation dataset\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d426dbc8c68c4981ab2ef979e5c5c688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 80\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 80\n",
      "  Total train batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2788054f089c422fb6518f9e0c81dd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7f3b422a514fc2bbc80682204ad5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dataset_seed in range(7):\n",
    "    initial_train_subset = create_random_subset(train_dataset, dataset_config, num_samples=samples_per_cycle, seed=dataset_seed)\n",
    "    run_hyperparam_search(initial_train_subset)\n",
    "#samples_per_cycle = 4* samples_per_cycle # larger dataset\n",
    "#for dataset_seed in range(7):\n",
    "#    initial_train_subset = create_random_subset(train_dataset, dataset_config, num_samples=samples_per_cycle, seed=dataset_seed)\n",
    "#    run_hyperparam_search(initial_train_subset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
