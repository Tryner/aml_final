{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "%cd /content/drive/MyDrive/aml_final/aml_final/\n",
    "! git pull\n",
    "! pip install setfit\n",
    "'''\n",
    "from datasets import load_dataset, Dataset\n",
    "from setfit import SetFitModel, Trainer, TrainingArguments\n",
    "import torch, gc\n",
    "\n",
    "from data.dataset_config import DatasetConfig\n",
    "from train.active_learning import ActiveTrainer, create_random_subset, add_random_samples\n",
    "from train.active_learning_config import ActiveLearningConfig\n",
    "from train.reporter import Reporter\n",
    "from train.metrics import camprehesive_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 6 #change\n",
    "samples_per_cycle = num_classes * 2\n",
    "dataset_name = \"dair-ai/emotion\" #change\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\" #change\n",
    "dataset = load_dataset(dataset_name) #change\n",
    "dataset_config = DatasetConfig(text_column=\"text\", num_classes=num_classes) #change?\n",
    "\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"validation\"]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset_name = dataset_name.replace(\"/\", \"_\") # prevent file system errors\n",
    "final_reporter = Reporter(dataset_name + \"_final.csv\", label_column=dataset_config.label_column)\n",
    "cycle_reporter = Reporter(dataset_name + \"_cycle.csv\", report_train_args=False, label_column=dataset_config.label_column) # not used\n",
    "def after_train_callback(trainer: Trainer, dataset: Dataset, run_id: int):\n",
    "    cycle_reporter.report(trainer=trainer, dataset=dataset, run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starts one run of active learning\n",
    "cycle_train_args = TrainingArguments(num_iterations=10, num_epochs=(1, 8))\n",
    "final_train_args = TrainingArguments(num_iterations=20, num_epochs=(1, 16))\n",
    "run_id = 0\n",
    "\n",
    "def run_train(initial_train_subset: Dataset, active_learning_config: ActiveLearningConfig, **kwargs):\n",
    "    global run_id\n",
    "    trainer = ActiveTrainer(\n",
    "        full_train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        initial_train_subset=initial_train_subset,\n",
    "        train_args=cycle_train_args,\n",
    "        active_learning_config=active_learning_config, \n",
    "        dataset_config=dataset_config,\n",
    "        metric=camprehesive_metrics,\n",
    "        run_id=run_id,\n",
    "        final_model_train_args=final_train_args,\n",
    "        #after_train_callback=after_train_callback, #just slows down training\n",
    "    )\n",
    "    if active_learning_config.active_sampling_strategy == \"random\": # speed up training, just train one cycle\n",
    "        trainer.train_subset = add_random_samples(train_dataset, initial_train_subset, len(initial_train_subset) + (samples_per_cycle * active_learning_config.active_learning_cycles), dataset_config, seed=run_id)\n",
    "        t = trainer.run_training(final_model=True)\n",
    "    else:\n",
    "        t = trainer.train()\n",
    "    final_reporter.report(\n",
    "        trainer=t, \n",
    "        dataset=trainer.train_subset, \n",
    "        active_learning_config=active_learning_config, \n",
    "        dataset_name=dataset_name, run_id=run_id, **kwargs\n",
    "        )\n",
    "    run_id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hyperparam_search(initial_train_subset: Dataset):\n",
    "    for setting in [\"random\", \"max_entropy\", \"max_entropy_balanced\"]:\n",
    "        if setting == \"random\":\n",
    "            strategy =  \"random\"\n",
    "            balance = 0.0\n",
    "        elif setting == \"max_entropy\":\n",
    "            strategy =  \"max_entropy\"\n",
    "            balance = 0.0\n",
    "        elif setting == \"max_entropy_balanced\":\n",
    "            strategy =  \"max_entropy\"\n",
    "            balance = 0.25\n",
    "        for unlabeled_samples in [10 * samples_per_cycle, 50 * samples_per_cycle]:\n",
    "            unlabeled_samples = min(unlabeled_samples, 1000)\n",
    "            config = ActiveLearningConfig(samples_per_cycle=samples_per_cycle, active_sampling_strategy=strategy, balancing_factor=balance, unlabeled_samples=unlabeled_samples, model_name=model_name)\n",
    "            run_train(initial_train_subset, config, setting=setting)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_seed in range(5):\n",
    "    initial_train_subset = create_random_subset(train_dataset, dataset_config, num_samples=samples_per_cycle, seed=dataset_seed)\n",
    "    run_hyperparam_search(initial_train_subset)\n",
    "samples_per_cycle = 4* samples_per_cycle # larger dataset\n",
    "for dataset_seed in range(5):\n",
    "    initial_train_subset = create_random_subset(train_dataset, dataset_config, num_samples=samples_per_cycle, seed=dataset_seed)\n",
    "    run_hyperparam_search(initial_train_subset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
