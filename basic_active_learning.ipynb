{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom google.colab import drive\\ndrive.mount(\\'/content/drive\\', force_remount=True)\\n%cd /content/drive/MyDrive/aml_final\\n#with open(\"github_token.txt\", \"r\") as f:\\n#  token = f.read()\\n#! git clone https://{token}@github.com/Tryner/aml_final.git #clone repo\\n%cd aml_final/\\n! git pull\\n! pip install setfit\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Colab\n",
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "%cd /content/drive/MyDrive/aml_final\n",
    "#with open(\"github_token.txt\", \"r\") as f:\n",
    "#  token = f.read()\n",
    "#! git clone https://{token}@github.com/Tryner/aml_final.git #clone repo\n",
    "%cd aml_final/\n",
    "! git pull\n",
    "! pip install setfit\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from setfit import SetFitModel, Trainer, TrainingArguments\n",
    "\n",
    "from train.active_learning import ActiveTrainer\n",
    "from train.active_learning_config import ActiveLearningConfig\n",
    "from data.dataset_config import DatasetConfig\n",
    "from data.load_datasets import select_dataset, load\n",
    "from train.reporter import Reporter\n",
    "from train.metrics import camprehesive_metrics\n",
    "import time\n",
    "from os import path, mkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select dataset:\n",
      "1. sst2\n",
      "2. french\n",
      "3. spanish\n",
      "4. indonesian\n"
     ]
    }
   ],
   "source": [
    "dataset_choice = select_dataset()\n",
    "dataset_name, dataset = load(dataset_choice)\n",
    "active_learning_config = ActiveLearningConfig(samples_per_cycle=2, unlabeled_samples=20, balancing_factor=0.5) # speed up training, not advisable\n",
    "dataset_config = DatasetConfig()\n",
    "train_args = TrainingArguments(num_epochs=1) #speed up training, not advisable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "run_description=\"result_files/\"+dataset_name+\"_\"+str(device)+\"/\"\n",
    "if not(path.isdir(run_description)):\n",
    "    mkdir(run_description)\n",
    "\n",
    "final_reporter = Reporter(run_description+\"final.csv\", label_column=dataset_config.label_column)\n",
    "cycle_reporter = Reporter(run_description+\"cycle.csv\", report_train_args=False, label_column=dataset_config.label_column)\n",
    "\n",
    "def model_init():\n",
    "    return SetFitModel.from_pretrained(active_learning_config.model_name, use_differentiable_head=True, head_params={\"out_features\": dataset_config.num_classes}).to(device)\n",
    "def after_train_callback(trainer: Trainer, dataset: Dataset, run_id: int):\n",
    "    cycle_reporter.report(trainer=trainer, dataset=dataset, run_id=run_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 2/2 [00:00<00:00, 333.38 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying column mapping to the training dataset\n",
      "Applying column mapping to the evaluation dataset\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Map: 100%|██████████| 2/2 [00:00<?, ? examples/s]\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 1\n",
      "  Total train batch size = 4\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A                                          \n",
      "100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n",
      "The `max_length` is `None`. Using the maximum acceptable length according to the current model body: 512.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.3507, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 1.9495, 'train_samples_per_second': 8.207, 'train_steps_per_second': 0.513, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [00:04<00:00,  4.56s/it]\n",
      "***** Running evaluation *****\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'result_files/twitter_humor_cpu/cycle.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 13\u001b[0m\n\u001b[0;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ActiveTrainer(\n\u001b[0;32m      3\u001b[0m model_init\u001b[38;5;241m=\u001b[39mmodel_init, \n\u001b[0;32m      4\u001b[0m full_train_dataset\u001b[38;5;241m=\u001b[39mdataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m metric\u001b[38;5;241m=\u001b[39mcamprehesive_metrics\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m start_time\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 13\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m execution_time\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     15\u001b[0m final_reporter\u001b[38;5;241m.\u001b[39mreport(\n\u001b[0;32m     16\u001b[0m     trainer\u001b[38;5;241m=\u001b[39mt, \n\u001b[0;32m     17\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtrain_subset, \n\u001b[0;32m     18\u001b[0m     active_learning_config\u001b[38;5;241m=\u001b[39mactive_learning_config, \n\u001b[0;32m     19\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39mdataset_name, run_id\u001b[38;5;241m=\u001b[39mrun_id, execution_time\u001b[38;5;241m=\u001b[39mexecution_time, device\u001b[38;5;241m=\u001b[39mdevice \u001b[38;5;66;03m#kwars, so you can put anything here\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\UTENTE\\Documents\\aml_final\\train\\active_learning.py:74\u001b[0m, in \u001b[0;36mActiveTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Trainer: \n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_learning_config\u001b[38;5;241m.\u001b[39mactive_learning_cycles):\n\u001b[1;32m---> 74\u001b[0m         trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m         sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_sentences(trainer\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m     76\u001b[0m         labeled_sentences: Dataset \u001b[38;5;241m=\u001b[39m label_sentences(sentences, labeled_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_train_dataset, text_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_config\u001b[38;5;241m.\u001b[39mtext_column)\n",
      "File \u001b[1;32mc:\\Users\\UTENTE\\Documents\\aml_final\\train\\active_learning.py:120\u001b[0m, in \u001b[0;36mActiveTrainer.run_training\u001b[1;34m(self, final_model)\u001b[0m\n\u001b[0;32m    111\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    112\u001b[0m     model_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_init,\n\u001b[0;32m    113\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_subset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m     column_mapping\u001b[38;5;241m=\u001b[39m{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_config\u001b[38;5;241m.\u001b[39mtext_column: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_config\u001b[38;5;241m.\u001b[39mlabel_column: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    118\u001b[0m )\n\u001b[0;32m    119\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_train_callback: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_train_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer\n",
      "Cell \u001b[1;32mIn[19], line 10\u001b[0m, in \u001b[0;36mafter_train_callback\u001b[1;34m(trainer, dataset, run_id)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mafter_train_callback\u001b[39m(trainer: Trainer, dataset: Dataset, run_id: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mcycle_reporter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\UTENTE\\Documents\\aml_final\\train\\reporter.py:72\u001b[0m, in \u001b[0;36mReporter.report\u001b[1;34m(self, trainer, dataset, active_learning_config, **other_params)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(duplicates) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuplicated keys: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(duplicates))\n\u001b[1;32m---> 72\u001b[0m     \u001b[43mwrite_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#first call, so write column names\u001b[39;00m\n\u001b[0;32m     74\u001b[0m data \u001b[38;5;241m=\u001b[39m other_params \u001b[38;5;241m|\u001b[39m metrics \u001b[38;5;241m|\u001b[39m dataset_description \u001b[38;5;241m|\u001b[39m active_learning_config \u001b[38;5;241m|\u001b[39m train_args \u001b[38;5;66;03m# dict Union\u001b[39;00m\n\u001b[0;32m     75\u001b[0m write_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_name, column_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_names, values\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\Users\\UTENTE\\Documents\\aml_final\\train\\reporter.py:12\u001b[0m, in \u001b[0;36mwrite_csv\u001b[1;34m(file_name, data, delimmiter)\u001b[0m\n\u001b[0;32m     10\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, data) \u001b[38;5;66;03m# ensure that we have strings\u001b[39;00m\n\u001b[0;32m     11\u001b[0m line \u001b[38;5;241m=\u001b[39m delimmiter\u001b[38;5;241m.\u001b[39mjoin(data) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     13\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(line)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'result_files/twitter_humor_cpu/cycle.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "for run_id in range(2):\n",
    "    trainer = ActiveTrainer(\n",
    "    model_init=model_init, \n",
    "    full_train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    train_args=train_args,\n",
    "    active_learning_config=active_learning_config, \n",
    "    dataset_config=dataset_config,\n",
    "    after_train_callback=after_train_callback,\n",
    "    metric=camprehesive_metrics\n",
    "    )\n",
    "    start_time=time.time()\n",
    "    t = trainer.train()\n",
    "    execution_time=time.time() - start_time\n",
    "    final_reporter.report(\n",
    "        trainer=t, \n",
    "        dataset=trainer.train_subset, \n",
    "        active_learning_config=active_learning_config, \n",
    "        dataset_name=dataset_name, run_id=run_id, execution_time=execution_time, device=device #kwars, so you can put anything here\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
